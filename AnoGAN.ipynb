{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.utils.generic_utils import Progbar\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, BatchNormalization, LeakyReLU, Reshape, Conv2DTranspose, \\\n",
    "    Flatten\n",
    "from keras.layers.core import Activation\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import math, cv2\n",
    "import numpy as np\n",
    "#import cupy as np\n",
    "import os\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# Generator\n",
    "class Generator(object):\n",
    "    def __init__(self, input_dim, image_shape):\n",
    "        INITIAL_CHANNELS = 128\n",
    "        # 生成した画像の1/4をsizeとする\n",
    "        INITIAL_SIZE = 25\n",
    "\n",
    "        inputs = Input((input_dim,))\n",
    "        fc1 = Dense(input_dim=input_dim, units=INITIAL_CHANNELS * INITIAL_SIZE * INITIAL_SIZE)(inputs)\n",
    "        fc1 = BatchNormalization()(fc1)\n",
    "        fc1 = LeakyReLU(0.2)(fc1)\n",
    "        fc2 = Reshape((INITIAL_SIZE, INITIAL_SIZE, INITIAL_CHANNELS),\n",
    "                      input_shape=(INITIAL_CHANNELS * INITIAL_SIZE * INITIAL_SIZE,))(fc1)\n",
    "        up1 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(fc2)\n",
    "        conv1 = Conv2D(64, (3, 3), padding='same')(up1)\n",
    "        conv1 = BatchNormalization()(conv1)\n",
    "        conv1 = Activation('relu')(conv1)\n",
    "        up2 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv1)\n",
    "        conv2 = Conv2D(image_shape[2], (5, 5), padding='same')(up2)\n",
    "        outputs = Activation('tanh')(conv2)\n",
    "\n",
    "        self.model = Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(object):\n",
    "    def __init__(self, input_shape):\n",
    "        inputs = Input(input_shape)\n",
    "        conv1 = Conv2D(64, (5, 5), padding='same')(inputs)\n",
    "        conv1 = LeakyReLU(0.2)(conv1)\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        conv2 = Conv2D(128, (5, 5), padding='same')(pool1)\n",
    "        conv2 = LeakyReLU(0.2)(conv2)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        fc1 = Flatten()(pool2)\n",
    "        fc1 = Dense(1)(fc1)\n",
    "        outputs = Activation('sigmoid')(fc1)\n",
    "\n",
    "        self.model = Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "\n",
    "# DCGAN\n",
    "class DCGAN(object):\n",
    "    def __init__(self, input_dim, image_shape):\n",
    "        self.input_dim = input_dim\n",
    "        self.d = Discriminator(image_shape).get_model()\n",
    "        self.g = Generator(input_dim, image_shape).get_model()\n",
    "\n",
    "    def compile(self, g_optim, d_optim):\n",
    "        self.d.trainable = False\n",
    "        self.dcgan = Sequential([self.g, self.d])\n",
    "        self.dcgan.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "        self.d.trainable = True\n",
    "        self.d.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "\n",
    "    def train(self, epochs, batch_size, X_train):\n",
    "        g_losses = []\n",
    "        d_losses = []\n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(X_train)\n",
    "            n_iter = X_train.shape[0] // batch_size\n",
    "            progress_bar = Progbar(target=n_iter)\n",
    "            for index in range(n_iter):\n",
    "                # create random noise -> N latent vectors\n",
    "                noise = np.random.uniform(-1, 1, size=(batch_size, self.input_dim))\n",
    "\n",
    "                # load real data & generate fake data\n",
    "                image_batch = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "                for i in range(batch_size):\n",
    "                    if np.random.random() > 0.5:\n",
    "                        image_batch[i] = np.fliplr(image_batch[i])\n",
    "                    if np.random.random() > 0.5:\n",
    "                        image_batch[i] = np.flipud(image_batch[i])\n",
    "                generated_images = self.g.predict(noise, verbose=0)\n",
    "\n",
    "                # attach label for training discriminator\n",
    "                X = np.concatenate((image_batch, generated_images))\n",
    "                y = np.array([1] * batch_size + [0] * batch_size)\n",
    "\n",
    "                # training discriminator\n",
    "                d_loss = self.d.train_on_batch(X, y)\n",
    "\n",
    "                # training generator\n",
    "                g_loss = self.dcgan.train_on_batch(noise, np.array([1] * batch_size))\n",
    "\n",
    "                progress_bar.update(index, values=[('g', g_loss), ('d', d_loss)])\n",
    "            g_losses.append(g_loss)\n",
    "            d_losses.append(d_loss)\n",
    "            \n",
    "            if (epoch + 1) % 1 == 0:\n",
    "                image = self.combine_images(generated_images)\n",
    "                image = (image + 1) / 2.0 * 255.0\n",
    "                cv2.imwrite('./results/' + str(epoch) + \".png\", image)\n",
    "            print('\\nEpoch' + str(epoch) + \" end\")\n",
    "\n",
    "            # save weights for each epoch\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                self.g.save('./weights/generator_' + str(epoch) + '.h5', True)\n",
    "                self.d.save('./weights/discriminator_' + str(epoch) + '.h5', True)\n",
    "        return g_losses, d_losses\n",
    "\n",
    "    def load_weights(self, g_weight, d_weight):\n",
    "        self.g.load_weights(g_weight)\n",
    "        self.d.load_weights(d_weight)\n",
    "\n",
    "    def combine_images(self, generated_images):\n",
    "        num = generated_images.shape[0]\n",
    "        width = int(math.sqrt(num))\n",
    "        height = int(math.ceil(float(num) / width))\n",
    "        shape = generated_images.shape[1:4]\n",
    "        image = np.zeros((height * shape[0], width * shape[1], shape[2]),\n",
    "                         dtype=generated_images.dtype)\n",
    "        for index, img in enumerate(generated_images):\n",
    "            i = int(index / width)\n",
    "            j = index % width\n",
    "            image[i * shape[0]:(i + 1) * shape[0], j * shape[1]:(j + 1) * shape[1], :] = img[:, :, :]\n",
    "        return image\n",
    "\n",
    "# AnoGAN\n",
    "def sum_of_residual(y_true, y_pred):\n",
    "    return K.sum(K.abs(y_true - y_pred))\n",
    "\n",
    "\n",
    "class ANOGAN(object):\n",
    "    def __init__(self, input_dim, g):\n",
    "        self.input_dim = input_dim\n",
    "        self.g = g\n",
    "        g.trainable = False\n",
    "        # Input layer cann't be trained. Add new layer as same size & same distribution\n",
    "        anogan_in = Input(shape=(input_dim,))\n",
    "        g_in = Dense((input_dim), activation='tanh', trainable=True)(anogan_in)\n",
    "        g_out = g(g_in)\n",
    "        self.model = Model(inputs=anogan_in, outputs=g_out)\n",
    "        self.model_weight = None\n",
    "\n",
    "    def compile(self, optim):\n",
    "        self.model.compile(loss=sum_of_residual, optimizer=optim)\n",
    "        K.set_learning_phase(0)\n",
    "\n",
    "    def compute_anomaly_score(self, x, iterations=300):\n",
    "        z = np.random.uniform(-1, 1, size=(1, self.input_dim))\n",
    "\n",
    "        # learning for changing latent\n",
    "        loss = self.model.fit(z, x, batch_size=1, epochs=iterations, verbose=0)\n",
    "        loss = loss.history['loss'][-1]\n",
    "        similar_data = self.model.predict_on_batch(z)\n",
    "\n",
    "        return loss, similar_data\n",
    "\n",
    "\n",
    "# train\n",
    "if __name__ == '__main__':\n",
    "    batch_size = 32\n",
    "    epochs = 500\n",
    "    input_dim = 100\n",
    "    g_optim = Adam(lr=0.0001, beta_1=0.5, beta_2=0.999)\n",
    "    d_optim = Adam(lr=0.0001, beta_1=0.5, beta_2=0.999)\n",
    "\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "    TRAIN_IMAGE_PATH = \"./cats/\"\n",
    "    #x_IMAGE_PATH = \"./Floc_Image_rot/s24_196_under/test/\"\n",
    "    #y_IMAGE_PATH = \"./Floc_Image_rot/s24_196_over/\"\n",
    "\n",
    "    img_list = os.listdir(TRAIN_IMAGE_PATH)\n",
    "    x_train = []\n",
    "    for img in img_list:\n",
    "        img = img_to_array(load_img(TRAIN_IMAGE_PATH + img, grayscale=False, target_size=(input_dim, input_dim, 3)))\n",
    "        img = (img.astype(np.float32) - 127.5) / 127.5\n",
    "        x_train.append(img)\n",
    "    x_train = np.array(x_train)\n",
    "\n",
    "    input_shape = x_train[0].shape\n",
    "\n",
    "    # train generator & discriminator\n",
    "    dcgan = DCGAN(input_dim, input_shape)\n",
    "    \n",
    "    #dcgan.load_weights('/Users/takaaki/Desktop/generator_1.h5',\n",
    "    #                                    '/Users/takaaki/Desktop/discriminator_1.h5')\n",
    "    \n",
    "    dcgan.compile(g_optim, d_optim)\n",
    "    g_losses, d_losses = dcgan.train(epochs, batch_size, x_train)\n",
    "    with open('loss.csv', 'w') as f:\n",
    "        for g_loss, d_loss in zip(g_losses, d_losses):\n",
    "            f.write(str(g_loss) + ',' + str(d_loss) + '\\n')\n",
    "\n",
    "\n",
    "# test\n",
    "K.set_learning_phase(1)\n",
    "\n",
    "\n",
    "def denormalize(X):\n",
    "    return ((X + 1.0) / 2.0 * 255.0).astype(dtype=np.uint8)\n",
    "\n",
    "\n",
    "# test\n",
    "if __name__ == '__main__':\n",
    "    iterations = 50\n",
    "    \n",
    "    # 画像サイズ\n",
    "    input_dim = 200\n",
    "    anogan_optim = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "    \n",
    "    y_IMAGE_PATH = \"./Floc_Image/s24_190_over/\"\n",
    "\n",
    "\n",
    "\n",
    "    img_list = os.listdir(y_IMAGE_PATH)\n",
    "    with open('./results/img_name.txt', 'a') as f:\n",
    "        f.write(str(img_list) + '\\n')\n",
    "\n",
    "    y_test = []\n",
    "    for img in img_list:\n",
    "        img = img_to_array(load_img(y_IMAGE_PATH + img, grayscale=True, target_size=(input_dim, input_dim, 1)))\n",
    "        # -1から1の範囲に正規化\n",
    "        img = (img.astype(np.float32) - 127.5) / 127.5\n",
    "        y_test.append(img)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "\n",
    "    # load weights\n",
    "    dcgan = DCGAN(input_dim, input_shape)\n",
    "    dcgan.load_weights('./weights/generator_1099.h5',\n",
    "                       './weights/discriminator_1099.h5')\n",
    "\n",
    "\n",
    "    for i, test_img in enumerate(y_test):\n",
    "        test_img = test_img[np.newaxis, :, :, :]\n",
    "        anogan = ANOGAN(input_dim, dcgan.g)\n",
    "        anogan.compile(anogan_optim)\n",
    "        anomaly_score, generated_img = anogan.compute_anomaly_score(test_img, iterations)\n",
    "\n",
    "        generated_img = denormalize(generated_img)\n",
    "        imgs = np.concatenate((denormalize(test_img[0]), generated_img[0]), axis=1)\n",
    "        cv2.imwrite('predict' + os.sep + str(int(anomaly_score)) + '_' + str(i) + '.png', imgs)\n",
    "        print(str(i) + ' %.2f' % anomaly_score)\n",
    "\n",
    "        with open('./results/y_test_score.txt', 'a') as f:\n",
    "            f.write(str(anomaly_score) + '\\n')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # plot histgram\n",
    "    import matplotlib.pyplot as plt\n",
    "    import csv\n",
    "\n",
    "    x = []\n",
    "    with open('C:/Users/Takaaki Ishii/Desktop/results/y_train_score.txt', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            row = int(float(row[0]))\n",
    "            x.append(row)\n",
    "    y = []\n",
    "    with open('C:/Users/Takaaki Ishii/Desktop/results/y_test_score.txt', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            row = int(float(row[0]))\n",
    "            y.append(row)\n",
    "\n",
    "    plt.title(\"Histgram of Score\")\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.ylabel(\"freq\")\n",
    "    plt.hist(x, bins=40, alpha=0.3, histtype='stepfilled', color='r', label=\"1\")\n",
    "    plt.hist(y, bins=40, alpha=0.3, histtype='stepfilled', color='b', label='9')\n",
    "    plt.legend(loc=1)\n",
    "    plt.savefig(\"C:/Users/Takaaki Ishii/Desktop/results/histgram.png\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 深層畳み込み生成対抗ネットワーク（DCGAN）と異常検出モデル（AnoGAN）を設定し、訓練する\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, BatchNormalization, LeakyReLU, Reshape, Conv2DTranspose, \\\n",
    "    Flatten, Activation\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "# Generatorクラス\n",
    "class Generator(object):\n",
    "    def __init__(self, input_dim, image_shape):\n",
    "        INITIAL_CHANNELS = 128\n",
    "        INITIAL_SIZE = 25  # 生成される画像の初期サイズ\n",
    "\n",
    "        inputs = Input((input_dim,))\n",
    "        fc1 = Dense(input_dim=input_dim, units=INITIAL_CHANNELS * INITIAL_SIZE * INITIAL_SIZE)(inputs)\n",
    "        fc1 = BatchNormalization()(fc1)\n",
    "        fc1 = LeakyReLU(0.2)(fc1)\n",
    "        fc2 = Reshape((INITIAL_SIZE, INITIAL_SIZE, INITIAL_CHANNELS))(fc1)\n",
    "        up1 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(fc2)\n",
    "        conv1 = Conv2D(64, (3, 3), padding='same')(up1)\n",
    "        conv1 = BatchNormalization()(conv1)\n",
    "        conv1 = Activation('relu')(conv1)\n",
    "        up2 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv1)\n",
    "        conv2 = Conv2D(image_shape[2], (5, 5), padding='same')(up2)\n",
    "        outputs = Activation('tanh')(conv2)\n",
    "\n",
    "        self.model = Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "# Discriminatorクラス\n",
    "class Discriminator(object):\n",
    "    def __init__(self, input_shape):\n",
    "        inputs = Input(input_shape)\n",
    "        conv1 = Conv2D(64, (5, 5), padding='same')(inputs)\n",
    "        conv1 = LeakyReLU(0.2)(conv1)\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        conv2 = Conv2D(128, (5, 5), padding='same')(pool1)\n",
    "        conv2 = LeakyReLU(0.2)(conv2)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        fc1 = Flatten()(pool2)\n",
    "        fc1 = Dense(1)(fc1)\n",
    "        outputs = Activation('sigmoid')(fc1)\n",
    "\n",
    "        self.model = Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "# DCGANクラス\n",
    "class DCGAN(object):\n",
    "    def __init__(self, input_dim, image_shape):\n",
    "        self.d = Discriminator(image_shape).model\n",
    "        self.g = Generator(input_dim, image_shape).model\n",
    "\n",
    "    def compile(self, g_optim, d_optim):\n",
    "        self.d.trainable = False\n",
    "        dcgan = Sequential([self.g, self.d])\n",
    "        dcgan.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "        self.d.trainable = True\n",
    "        self.d.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "\n",
    "# 主要な訓練プロセスとテストプロセスは大幅に簡略化されています。\n",
    "# 訓練プロセスでは、画像データを読み込み、DCGANモデルをインスタンス化し、オプティマイザを設定してモデルをコンパイルし、訓練を実行します。\n",
    "# AnoGANクラスやその他の関数は、異常検出のための追加機能を提供しますが、主要な訓練ループからは省略しています。\n",
    "\n",
    "# 重要なポイント:\n",
    "# - `Generator`は、ランダムノイズから画像を生成するネットワークです。\n",
    "# - `Discriminator`は、画像が本物か生成されたものかを識別するネットワークです。\n",
    "# - `DCGAN`は、これら二つのネットワークを組み合わせ、互いに競合させながら訓練します。\n",
    "# - コードの不要な部分（重複するインポート文や未使用の変数など）を削除し、日本語でのコメントを追加して、各クラスとメソッドの目的を明確にしました。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# DCGANの訓練プロセス\n",
    "# DCGANは、GeneratorとDiscriminatorという2つのネットワークで構成されます。G\n",
    "# eneratorはランダムノイズから画像を生成し、Discriminatorは画像が本物か生成されたものかを識別します。\n",
    "# 訓練プロセスでは、Generatorを改善してよりリアルな画像を生成し、Discriminatorを改善して本物と生成された画像をより正確に識別するようにします。\n",
    "\n",
    "# 訓練プロセスの実装\n",
    "def train(self, epochs, batch_size, X_train):\n",
    "    half_batch = int(batch_size / 2)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # バッチ単位での訓練\n",
    "        for _ in range(int(X_train.shape[0] / batch_size)):\n",
    "            \n",
    "            # 本物の画像のサンプリング\n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # 偽の画像の生成\n",
    "            noise = np.random.normal(0, 1, (half_batch, self.input_dim))\n",
    "            gen_imgs = self.g.predict(noise)\n",
    "\n",
    "            # Discriminatorの訓練\n",
    "            d_loss_real = self.d.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = self.d.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # Generatorの訓練\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.input_dim))\n",
    "            valid_y = np.array([1] * batch_size)\n",
    "            g_loss = self.dcgan.train_on_batch(noise, valid_y)\n",
    "\n",
    "            # 進捗の表示\n",
    "            print(\"%d [D loss: %f] [G loss: %f]\" % (epoch, d_loss, g_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 異常検出（AnoGAN）\n",
    "# AnoGANは、訓練済みのGeneratorを使用して、入力画像に似た画像を生成することにより、異常検出を行います。\n",
    "# 異常検出のプロセスは、入力画像に最も近い潜在空間のベクトルを見つけることです。\n",
    "\n",
    "# AnoGANの異常検出プロセス\n",
    "def compute_anomaly_score(self, x, iterations=500):\n",
    "    z = np.random.normal(size=(1, self.input_dim))\n",
    "    z = K.variable(z)\n",
    "    x = K.variable(x)\n",
    "    \n",
    "    # 損失関数を定義\n",
    "    loss = K.mean(K.abs(self.g(z) - x))\n",
    "    \n",
    "    # 損失に対するzの勾配を計算\n",
    "    grads = K.gradients(loss, [z])[0]\n",
    "    \n",
    "    # zを更新する関数\n",
    "    iterate = K.function([z], [loss, grads])\n",
    "    \n",
    "    # 勾配降下法でzを更新\n",
    "    for i in range(iterations):\n",
    "        loss_value, grads_value = iterate([z])\n",
    "        z -= grads_value * 0.01\n",
    "        \n",
    "    z = K.eval(z)\n",
    "    g_z = K.eval(self.g(K.variable(z)))\n",
    "    \n",
    "    # 異常スコア（入力画像と生成画像の差の絶対値の和）を計算\n",
    "    anomaly_score = np.sum(np.abs(g_z - x))\n",
    "    return anomaly_score, g_z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練と異常検出の実行\n",
    "# 訓練済みのモデルを使用して異常検出を行います。ここでは、訓練プロセスの後に異常検出を行う一連のステップを実行します。\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # データの準備\n",
    "    X_train = # ここで訓練データを準備\n",
    "\n",
    "    # DCGANのインスタンス化とコンパイル\n",
    "    dcgan = DCGAN(input_dim, image_shape)\n",
    "    dcgan.compile(g_optim, d_optim)\n",
    "    \n",
    "    # DCGANの訓練\n",
    "    g_losses, d_losses = dcgan.train(epochs, batch_size, X_train)\n",
    "    \n",
    "    # 異常検出の実行\n",
    "    test_img = # ここでテスト画像を準備\n",
    "    anogan = ANOGAN(input_dim, dcgan.g)\n",
    "    anogan.compile(anogan_optim)\n",
    "    anomaly_score, similar_img = anogan.compute_anomaly_score(test_img)\n",
    "    \n",
    "    print(\"Anomaly Score:\", anomaly_score)\n",
    "    # 異常スコアと類似画像の表示など\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
