{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# VGG19をベースとした転移学習を使用して、特定の画像分類タスク（二値分類）を行うためのものです。指定されたディレクトリから画像データを読み込み、前処理を行った後、モデルを構築し、訓練し、評価\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras import backend as K\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras.activations import relu, sigmoid\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# データの読み込みと前処理\n",
    "img_size = 50\n",
    "channels = 3\n",
    "input_shape = (img_size, img_size, channels)\n",
    "\n",
    "def load_images(image_dir, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for f in os.listdir(image_dir):\n",
    "        img = img_to_array(load_img(os.path.join(image_dir, f), target_size=input_shape[:2]))\n",
    "        images.append(img)\n",
    "        labels.append(label)\n",
    "    images = np.array(images).astype('float32') / 255.0\n",
    "    labels = np_utils.to_categorical(labels, 2)\n",
    "    return images, labels\n",
    "\n",
    "# データセットの読み込み\n",
    "path1_under = \"/path/to/train2/s24_190_under/train\"\n",
    "path1_over = \"/path/to/train/s24_190_over/train\"\n",
    "path2_under = \"/path/to/train2/s24_190_under/test\"\n",
    "path2_over = \"/path/to/train/s24_190_over/test\"\n",
    "\n",
    "x_train, y_train = load_images(path1_under, 0)\n",
    "x_train_over, y_train_over = load_images(path1_over, 1)\n",
    "x_train = np.concatenate((x_train, x_train_over))\n",
    "y_train = np.concatenate((y_train, y_train_over))\n",
    "\n",
    "x_val, y_val = load_images(path2_under, 0)\n",
    "x_val_over, y_val_over = load_images(path2_over, 1)\n",
    "x_val = np.concatenate((x_val, x_val_over))\n",
    "y_val = np.concatenate((y_val, y_val_over))\n",
    "\n",
    "\n",
    "# モデルの構築\n",
    "def cross_entropy_error(y_true, y_pred):\n",
    "    return -K.sum(y_true * K.log(y_pred) * 1.2 + (1 - y_true) * K.log(1 - y_pred))\n",
    "\n",
    "# モデルの構築\n",
    "def build_model(input_shape):\n",
    "    WEIGHTS_PATH_NO_TOP = 'vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    base_model = VGG19(weights=WEIGHTS_PATH_NO_TOP, include_top=False, input_tensor=input_tensor)\n",
    "    x = Flatten()(base_model.output)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(2, activation='sigmoid')(x)\n",
    "    model = Model(inputs=input_tensor, outputs=x)\n",
    "\n",
    "    # カスタム損失関数を使用してモデルをコンパイル\n",
    "    model.compile(optimizer=RMSprop(lr=0.001), loss=cross_entropy_error, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model(input_shape=(img_size, img_size, channels))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "# 訓練\n",
    "epochs = 2\n",
    "batch_size = 8\n",
    "file_name = \"model_output\"\n",
    "\n",
    "# コールバックの設定\n",
    "model_checkpoint = ModelCheckpoint(filepath=os.path.join(file_name, 'model_{epoch:02d}.h5'), save_best_only=True, verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1)\n",
    "\n",
    "# モデルの訓練\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_val, y_val), callbacks=[model_checkpoint, early_stop, reduce_lr])\n",
    "\n",
    "# 結果の保存\n",
    "pd.DataFrame(history.history).to_csv(file_name + '/history.csv')\n",
    "model.save(file_name + \"/final_model.h5\")\n",
    "\n",
    "\n",
    "\n",
    "# 損失のグラフ\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 精度のグラフ\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(file_name + '/training_validation_results.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
